{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numba\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.ndimage.interpolation as sni\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d as conv2d\n",
    "%matplotlib inline\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def D_ReLU(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def L_ReLU(x):\n",
    "    return np.where(x < 0, 0.001 * x, x)\n",
    "\n",
    "def D_L_ReLU(x):\n",
    "    return np.where(x < 0, 0.001, 1)\n",
    "\n",
    "def I_L_ReLU(x):\n",
    "    return np.where(x < 0, 1000 * x, x)\n",
    "\n",
    "def softmax(x):\n",
    "    for i in range(x.shape[1]):\n",
    "        x[:, i] -= np.max(x[:, i])\n",
    "        x[:, i] = np.exp(x[:, i])\n",
    "        x[:, i] = x[:, i] / np.sum(x[:, i])\n",
    "    return x\n",
    "\n",
    "# 交叉熵损失函数\n",
    "# cross-entropy cost function\n",
    "def Loss(predicted, target):\n",
    "    return softmax(predicted) - target\n",
    "\n",
    "def mkdir(path):\n",
    "    folder = os.path.exists(path)\n",
    "    #判断是否存在文件夹如果不存在则创建为文件夹 If the folder does not exist, create a new folder\n",
    "    if not folder:                   \n",
    "        os.makedirs(path)            \n",
    "        \n",
    "@numba.njit\n",
    "def RandomNoise(x):\n",
    "    x = x.reshape(-1) # flat view\n",
    "    for ii in range(len(x)):\n",
    "        x[ii] += random.uniform(-0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义卷积函数\n",
    "# Convolution function\n",
    "\n",
    "def convolution2D(input_array, kernel, stride):\n",
    "    input_row, input_column = input_array.shape #行 列\n",
    "    kernel_row, kernel_column = kernel.shape\n",
    "    new_row = (input_row - kernel_row) // stride + 1\n",
    "\n",
    "    new_column = (input_column - kernel_column) // stride + 1\n",
    "    new_array = np.zeros((new_row, new_column))\n",
    "\n",
    "    for j in range(new_row):\n",
    "        for k in range(new_column):\n",
    "            new_array[j, k] = np.sum(input_array[j * stride : j * stride + kernel_row, \\\n",
    "                                                        k * stride : k * stride + kernel_column] * kernel)\n",
    "    \n",
    "    return new_array\n",
    "\n",
    "\n",
    "# 零填充\n",
    "# zero padding\n",
    "def padding(input_array, n):\n",
    "    if n == 0:\n",
    "        return input_array\n",
    "    else:\n",
    "        input_array_size = input_array.shape\n",
    "        output_array = np.zeros((input_array_size[0], input_array_size[1] + 2 * n, input_array_size[2] + 2 * n, input_array_size[3]))\n",
    "        output_array[:, n : n + input_array_size[1], n : n + input_array_size[2], :] = input_array\n",
    "        return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积层\n",
    "# Convolution layer\n",
    "class ConvLayer:\n",
    "    def __init__(self, input_shape, kernel_shape, stride, zero_padding, learning_rate, \\\n",
    "                 residual_error_flag, first_layer_flag, lock_flag, name):\n",
    "                \n",
    "        # 样本数 行 列 通道数\n",
    "        # sample size， row, column. channel\n",
    "        self.input_shape = input_shape\n",
    "        self.input_number = input_shape[0] #minibatch\n",
    "        \n",
    "        self.input_row = input_shape[1] + 2 * zero_padding\n",
    "        self.input_column = input_shape[2] + 2 * zero_padding\n",
    "        self.input_channel = input_shape[3] \n",
    "                   \n",
    "        self.kernel_shape = kernel_shape \n",
    "        #行 列 输入通道数 卷积核通道数\n",
    "        #row, column, input channel, kernel\n",
    "        \n",
    "        self.kernel_size = kernel_shape[0]\n",
    "        self.kernel_channel = kernel_shape[3]\n",
    "        self.stride = stride \n",
    "        self.zero_padding = zero_padding\n",
    "        self.learning_rate = learning_rate * (1 / self.input_number)\n",
    "        self.residual_error_flag = residual_error_flag\n",
    "        self.first_layer_flag = first_layer_flag\n",
    "        self.lock_flag = lock_flag\n",
    "        self.name = name\n",
    "        \n",
    "        self.conv_out_row = (self.input_row - self.kernel_size) // self.stride + 1     \n",
    "        self.conv_out_column = (self.input_column - self.kernel_size) // self.stride + 1\n",
    "        #self.conv_out_array = np.zeros((self.input_number, self.conv_out_row, self.conv_out_column, self.kernel_channel))\n",
    "        \n",
    "        # kaiming高斯初始化\n",
    "        # kaiming Gaussian initialization\n",
    "        #self.kernel = np.random.normal(0.0, pow(2 / (self.kernel_size * self.kernel_size * self.kernel_channel), 0.5), \\\n",
    "                                           #(self.kernel_size, self.kernel_size, self.kernel_channel))\n",
    "        \n",
    "        # xavier初始化\n",
    "        # xavier initialization\n",
    "        input_number = self.input_row * self.input_column * self.input_channel\n",
    "        conv_out_number = self.conv_out_row * self.conv_out_column * self.kernel_channel\n",
    "        \n",
    "        temp_number = pow(6 / (input_number + conv_out_number), 0.5)\n",
    "        \n",
    "        self.kernel = np.random.uniform(-temp_number, temp_number, kernel_shape)\n",
    "        #self.kernel_error = np.zeros(kernel_shape)\n",
    "        \n",
    "        self.activation_function = lambda x : L_ReLU(x)\n",
    "        self.D_activation_function = lambda x : D_L_ReLU(x)\n",
    "        \n",
    "    # 正向查询\n",
    "    # forward propagation\n",
    "    def fp(self, input_array):\n",
    "        \n",
    "        input_array = padding(input_array, self.zero_padding)\n",
    "        \n",
    "        self.conv_out_array = np.zeros((input_array.shape[0], self.conv_out_row, self.conv_out_column, self.kernel_channel))\n",
    "        # 卷积操作 Convolution\n",
    "        for n in range(input_array.shape[0]):\n",
    "            for i in range(self.kernel_channel):\n",
    "                #temp_kernel = np.dstack([self.kernel[:, :, i]] * self.input_channel)\n",
    "                for j in range(self.conv_out_row):\n",
    "                    for k in range(self.conv_out_column):\n",
    "                        self.conv_out_array[n, j, k, i] = \\\n",
    "                        np.sum(input_array[n, j * self.stride : j * self.stride + self.kernel_size, \\\n",
    "                                           k * self.stride : k * self.stride + self.kernel_size, :] * self.kernel[:, :, :, i])\n",
    "        if self.residual_error_flag:\n",
    "            return self.activation_function(self.conv_out_array) + self.conv_out_array\n",
    "                    \n",
    "        return self.activation_function(self.conv_out_array)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 反向查询\n",
    "    # back propagation\n",
    "    def bp(self, input_array, error):\n",
    "        \n",
    "        input_array = padding(input_array, self.zero_padding)\n",
    "        \n",
    "        if self.residual_error_flag:\n",
    "            conv_out_error = error * (self.D_activation_function(self.conv_out_array) + 1)\n",
    "        else:\n",
    "            conv_out_error = error * self.D_activation_function(self.conv_out_array)\n",
    "\n",
    "        new_array_row = (self.conv_out_row - 1) * self.stride + 1\n",
    "        new_array_column = (self.conv_out_column - 1) * self.stride + 1\n",
    "            \n",
    "        if self.stride != 1:     \n",
    "            \n",
    "            \n",
    "            new_array = np.zeros((self.input_number, new_array_row, new_array_column, self.kernel_channel))\n",
    "            for n in range(self.input_number):\n",
    "                for i in range(self.conv_out_row):\n",
    "                    for j in range(self.conv_out_column):\n",
    "                        new_array[n, i * self.stride , j * self.stride , :] = conv_out_error[n, i, j, :]\n",
    "        else:\n",
    "            new_array = conv_out_error\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.first_layer_flag :\n",
    "            conv_error = None\n",
    "        else:\n",
    "            new_array_padding = padding(new_array, self.kernel_size - 1)\n",
    "            conv_error = np.zeros(input_array.shape)\n",
    "            for n in range(self.input_number):\n",
    "                for i in range(self.kernel_channel):\n",
    "                    for j in range(self.input_channel):\n",
    "                        conv_error[n, :, :, j] += conv2d(new_array_padding[n, :, :, i], self.kernel[:, :, j, i], mode='valid')\n",
    "            \n",
    "            conv_error = conv_error[:, self.zero_padding : self.input_row - self.zero_padding, \\\n",
    "                          self.zero_padding : self.input_column - self.zero_padding, :]\n",
    "            \n",
    "            \n",
    "        if not self.lock_flag:            \n",
    "            kernel_error = np.zeros(self.kernel_shape)\n",
    "\n",
    "            for n in range(self.input_number):\n",
    "                for i in range(self.kernel_channel):\n",
    "                    for j in range(self.input_channel):\n",
    "                        kernel_error[:, :, j, i] += convolution2D(input_array[n, :, :, j], new_array[n, :, :, i], 1)\n",
    "\n",
    "            self.kernel -= self.learning_rate * kernel_error     \n",
    "                     \n",
    "        return conv_error\n",
    "    \n",
    "    \n",
    "    def backQuery(self, error):\n",
    "        # 用于可视化反卷积 Used for visualization\n",
    "        # Matthew D. Zeiler , Rob Fergus. Visualizing and Understanding Convolutional Networks.European Conference on Computer Vision, 2014 – Springer.\n",
    "        \n",
    "        new_array_padding = padding(error, self.kernel_size - 1)\n",
    "\n",
    "        conv_error = np.zeros((1, self.input_row, self.input_column, self.input_channel))\n",
    "        for n in range(1):\n",
    "            for i in range(self.kernel_channel):\n",
    "                for j in range(self.input_channel):\n",
    "                    conv_error[n, :, :, j] += conv2d(new_array_padding[n, :, :, i], self.kernel[:, :, j, i], mode='valid')\n",
    "\n",
    "        conv_error = conv_error[:, self.zero_padding : self.input_row - self.zero_padding, \\\n",
    "                        self.zero_padding : self.input_column - self.zero_padding, :]\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "        return self.activation_function(conv_error)\n",
    "\n",
    "    \n",
    "    \n",
    "    def saveKernel(self, path):\n",
    "        path += self.name\n",
    "        mkdir(path)\n",
    "        for i in range(self.kernel_channel):\n",
    "            for j in range(self.input_channel):\n",
    "                filename = path + '\\\\kernel_' + str(i) + str(j) + '.txt'\n",
    "                np.savetxt(filename, self.kernel[:, :, j, i], fmt='%2.54f', delimiter=\",\")\n",
    "        \n",
    "    def loadKernel(self, path):\n",
    "        path += self.name\n",
    "        for i in range(self.kernel_channel):\n",
    "            for j in range(self.input_channel):\n",
    "                filename = path + '\\\\kernel_' + str(i) + str(j) + '.txt'\n",
    "                self.kernel[:, :, j, i] = np.loadtxt(filename, dtype=np.float64, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 池化层\n",
    "class PoolingLayer:\n",
    "\n",
    "    def __init__(self, input_shape, kernel_size, stride, mode):\n",
    "        \n",
    "        self.input_number, self.input_row, self.input_column, self.input_channel = input_shape #行 列 通道数\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.mode = mode\n",
    "               \n",
    "        self.pooling_out_row = (self.input_row - self.kernel_size) // self.stride + 1\n",
    "        self.pooling_out_column = (self.input_column - self.kernel_size) // self.stride + 1\n",
    "        #self.pooling_out_array = np.zeros((self.input_number, self.pooling_out_row, self.pooling_out_column, self.input_channel))\n",
    "        \n",
    "        self.pooling_error = np.zeros(input_shape)\n",
    "        \n",
    "        self.avgpooling_parameter = 1 / (kernel_size * kernel_size)\n",
    "        \n",
    "        # 位置矩阵， 保存最大池化的位置信息 Save location information of max pooling\n",
    "        self.location_array = np.zeros((self.input_number, self.pooling_out_row, self.pooling_out_column, \\\n",
    "                                        self.input_channel * 2)).astype(np.int32)\n",
    "        \n",
    "    # 正向查询\n",
    "    def fp(self, input_array):\n",
    "        \n",
    "        self.pooling_out_array = np.zeros((input_array.shape[0], self.pooling_out_row, self.pooling_out_column, self.input_channel))\n",
    "        # 池化操作\n",
    "        if self.mode == 'max':\n",
    "            for n in range(input_array.shape[0]):\n",
    "                for i in range(self.input_channel):\n",
    "                    for j in range(self.pooling_out_row):\n",
    "                        for k in range(self.pooling_out_column):\n",
    "                            temp_location = \\\n",
    "                            np.argmax(input_array[n, j * self.stride : j * self.stride + self.kernel_size, \\\n",
    "                                                  k * self.stride : k * self.stride + self.kernel_size, i])\n",
    "                            self.location_array[n, j, k, 2 * i] = j * self.stride + temp_location // self.kernel_size\n",
    "                            self.location_array[n, j, k, 2 * i + 1] = k * self.stride + temp_location % self.kernel_size\n",
    "                            self.pooling_out_array[n, j, k, i] = input_array[n, self.location_array[n, j, k, 2 * i], \\\n",
    "                                                                             self.location_array[n, j, k, 2 * i + 1], i]\n",
    "        elif self.mode == 'avg':\n",
    "            for n in range(input_array.shape[0]):\n",
    "                for i in range(self.input_channel):\n",
    "                    for j in range(self.pooling_out_row):\n",
    "                        for k in range(self.pooling_out_column):\n",
    "                            self.pooling_out_array[n, j, k, i] = self.avgpooling_parameter * \\\n",
    "                                        np.sum(input_array[n, j * self.stride : j * self.stride + self.kernel_size, \\\n",
    "                                                           k * self.stride : k * self.stride + self.kernel_size, i])\n",
    "\n",
    "        else:\n",
    "            raise Exception('mode Error')  \n",
    "            \n",
    "            \n",
    "            \n",
    "        return self.pooling_out_array\n",
    "    \n",
    "    def bp(self, error):\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            for n in range(self.input_number):\n",
    "                for i in range(self.input_channel):\n",
    "                    for j in range(self.pooling_out_row):\n",
    "                        for k in range(self.pooling_out_column):\n",
    "                             self.pooling_error[n, self.location_array[n, j, k, 2 * i], self.location_array[n, j, k, 2 * i + 1], i] = \\\n",
    "                                error[n, j, k, i]\n",
    "\n",
    "        elif self.mode == 'avg':\n",
    "            for n in range(self.input_number):\n",
    "                for i in range(self.input_channel):\n",
    "                    for j in range(self.pooling_out_row):\n",
    "                        for k in range(self.pooling_out_column):\n",
    "                             self.pooling_error[n, j * self.stride : j * self.stride + self.kernel_size, \\\n",
    "                                                k * self.stride : k * self.stride + self.kernel_size, i] += \\\n",
    "                                                self.avgpooling_parameter * error[n, j, k, i]\n",
    "        else:\n",
    "            raise Exception('mode Error') \n",
    "            \n",
    "        return self.pooling_error\n",
    "    def backQuery(self, error):\n",
    "        if self.mode == 'max':\n",
    "            for n in range(1):\n",
    "                for i in range(self.input_channel):\n",
    "                    for j in range(self.pooling_out_row):\n",
    "                        for k in range(self.pooling_out_column):\n",
    "                             self.pooling_error[n, self.location_array[n, j, k, 2 * i], self.location_array[n, j, k, 2 * i + 1], i] = \\\n",
    "                                error[n, j, k, i]\n",
    "        return self.pooling_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全连接层\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_nodes, output_nodes, learning_rate, mini_batch, residual_error_flag, lock_flag, name):\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.learning_rate = learning_rate * (1 / mini_batch)\n",
    "        self.mini_batch = mini_batch\n",
    "        self.residual_error_flag = residual_error_flag\n",
    "        self.lock_flag = lock_flag\n",
    "        self.name = name\n",
    "        \n",
    "        #self.bias = np.ones((1, 1))\n",
    "        \n",
    "        # 高斯初始化权重\n",
    "        #self.weight = np.random.normal(0.0, pow(2 / input_nodes, 0.5), (output_nodes, input_nodes))\n",
    "        \n",
    "        # xavier初始化\n",
    "        temp_number = pow(6 / (self.input_nodes + self.output_nodes), 0.5)\n",
    "        self.weight = np.random.uniform(-temp_number, temp_number, (self.output_nodes, self.input_nodes))\n",
    "        \n",
    "        # 激活函数\n",
    "        self.activation_function = lambda x : L_ReLU(x)\n",
    "        self.D_activation_function = lambda x : D_L_ReLU(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def fp(self, input_array):\n",
    "         \n",
    "        self.output_array = np.dot(self.weight, input_array)\n",
    "        \n",
    "        if self.residual_error_flag:\n",
    "            return self.activation_function(self.output_array) + self.output_array\n",
    "        \n",
    "        return self.activation_function(self.output_array)\n",
    "    \n",
    "    def bp(self, input_array, error):\n",
    "        \n",
    "        if self.residual_error_flag:\n",
    "            output_error = error * (self.D_activation_function(self.output_array) + 1)\n",
    "        else:\n",
    "            output_error = error * self.D_activation_function(self.output_array)\n",
    "        input_error = np.dot(self.weight.T, output_error)\n",
    "        \n",
    "        if not self.lock_flag:\n",
    "            self.weight -= self.learning_rate * np.dot(output_error, np.transpose(input_array))\n",
    "        \n",
    "        return input_error\n",
    "    \n",
    "    def saveWeight(self, path):\n",
    "        path += self.name\n",
    "        mkdir(path)\n",
    "        filename = path + '\\\\weight.txt'\n",
    "        np.savetxt(filename, self.weight, fmt='%2.54f', delimiter=\",\")\n",
    "        \n",
    "    def loadWeight(self, path):\n",
    "        path += self.name\n",
    "        filename = path + '\\\\weight.txt'\n",
    "        self.weight = np.loadtxt(filename, dtype=np.float64, delimiter=',')     \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全连接层\n",
    "# Same thing as FullyConnectedLayer\n",
    "class LastConnectedLayer:\n",
    "    def __init__(self, input_nodes, output_nodes, learning_rate, mini_batch, residual_error_flag, lock_flag, name):\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.learning_rate = learning_rate * (1 / mini_batch)\n",
    "        self.mini_batch = mini_batch\n",
    "        self.residual_error_flag = residual_error_flag\n",
    "        self.lock_flag = lock_flag\n",
    "        self.name = name\n",
    "        \n",
    "        #self.bias = np.ones((1, 1))\n",
    "        \n",
    "        # 高斯初始化权重\n",
    "        #self.weight = np.random.normal(0.0, pow(2 / input_nodes, 0.5), (output_nodes, input_nodes))\n",
    "        \n",
    "        # xavier初始化\n",
    "        temp_number = pow(6 / (self.input_nodes + self.output_nodes), 0.5)\n",
    "        self.weight = np.random.uniform(-temp_number, temp_number, (self.output_nodes, self.input_nodes))\n",
    "        \n",
    "        # 激活函数\n",
    "        self.activation_function = lambda x : L_ReLU(x)\n",
    "        self.D_activation_function = lambda x : D_L_ReLU(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def fp(self, input_array):\n",
    "         \n",
    "        self.output_array = np.dot(self.weight, input_array)\n",
    "              \n",
    "        return self.output_array\n",
    "    \n",
    "    def bp(self, input_array, error):\n",
    "        \n",
    "        input_error = np.dot(self.weight.T, error)\n",
    "        \n",
    "        if not self.lock_flag:\n",
    "            self.weight -= self.learning_rate * np.dot(error, np.transpose(input_array))\n",
    "        \n",
    "        return input_error\n",
    "    \n",
    "    def saveWeight(self, path):\n",
    "        path += self.name\n",
    "        mkdir(path)\n",
    "        filename = path + '\\\\weight.txt'\n",
    "        np.savetxt(filename, self.weight, fmt='%2.54f', delimiter=\",\")\n",
    "        \n",
    "    def loadWeight(self, path):\n",
    "        path += self.name\n",
    "        filename = path + '\\\\weight.txt'\n",
    "        self.weight = np.loadtxt(filename, dtype=np.float64, delimiter=',')     \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批标准化 Batch Normalization\n",
    "# 论文作者的更复杂，此程序简化了反向传播时的一些计算量\n",
    "# Reduced some computation of back propagation\n",
    "# Sergey Ioffe, Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.International Conference on Machine Learning\n",
    "\n",
    "class BatchNormalizationLayer2D:\n",
    "    def __init__(self, input_shape, learning_rate, lock_flag, name):\n",
    "        self.input_row, self.input_column = input_shape\n",
    "        self.learning_rate = learning_rate * (1 / self.input_column)\n",
    "        self.lock_flag = lock_flag\n",
    "        self.name = name\n",
    "        self.mean = np.zeros((self.input_row, 1))\n",
    "        self.variance = np.ones((self.input_row, 1))        \n",
    "        self.beta = np.zeros((self.input_row, 1))\n",
    "        self.gamma = np.ones((self.input_row, 1))\n",
    "        self.momentum = 0.999\n",
    "\n",
    "    def fp(self, input_array):\n",
    "        if not self.lock_flag:\n",
    "            if input_array.shape[1] != 1:\n",
    "                self.mean = self.momentum * self.mean + (1 - self.momentum) * input_array.mean(axis = 1).reshape(self.input_row, 1)\n",
    "                self.variance = self.momentum  * self.variance + \\\n",
    "                                          (1.0 - self.momentum) * input_array.var(axis = 1).reshape(self.input_row, 1)\n",
    "        #Xi\n",
    "        self.new_array = (input_array - self.mean) / np.sqrt(self.variance + 0.000001)\n",
    "        self.output_array = self.new_array * self.gamma + self.beta\n",
    "        return self.output_array\n",
    "    \n",
    "    def bp(self, error):\n",
    "        \n",
    "        new_array_error = error * self.gamma   \n",
    "        if not self.lock_flag:\n",
    "            self.gamma -= self.learning_rate * np.sum((error * self.new_array), axis=1).reshape(self.input_row, 1)        \n",
    "            self.beta -= self.learning_rate * np.sum(error, axis=1).reshape(self.input_row, 1)\n",
    "        input_error = new_array_error / np.sqrt(self.variance + 0.000001)\n",
    "        \n",
    "        return input_error\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    def saveBNParameter(self, path):\n",
    "        path += self.name\n",
    "        mkdir(path)\n",
    "        filename = path + '\\\\mean.txt'\n",
    "        np.savetxt(filename, self.mean.reshape(self.input_row, 1), fmt='%2.54f', delimiter=\",\")\n",
    "        filename = path + '\\\\variance.txt'\n",
    "        np.savetxt(filename, self.variance.reshape(self.input_row, 1), fmt='%2.54f', delimiter=\",\")\n",
    "        filename = path + '\\\\gamma.txt'\n",
    "        np.savetxt(filename, self.gamma.reshape(self.input_row, 1), fmt='%2.54f', delimiter=\",\")\n",
    "        filename = path + '\\\\beta.txt'\n",
    "        np.savetxt(filename, self.beta.reshape(self.input_row, 1), fmt='%2.54f', delimiter=\",\")\n",
    "    \n",
    "    def loadBNParameter(self, path):\n",
    "        path += self.name\n",
    "        filename = path + '\\\\mean.txt'\n",
    "        self.mean = np.loadtxt(filename, dtype=np.float64, delimiter=',').reshape(self.input_row, 1)     \n",
    "        filename = path + '\\\\variance.txt'\n",
    "        self.variance = np.loadtxt(filename, dtype=np.float64, delimiter=',').reshape(self.input_row, 1)     \n",
    "        filename = path + '\\\\gamma.txt'\n",
    "        self.gamma = np.loadtxt(filename, dtype=np.float64, delimiter=',').reshape(self.input_row, 1)     \n",
    "        filename = path + '\\\\beta.txt'\n",
    "        self.beta = np.loadtxt(filename, dtype=np.float64, delimiter=',').reshape(self.input_row, 1)     \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftMax层\n",
    "class SoftMax:\n",
    "    def fp(self, input_array):\n",
    "        return softmax(input_array)\n",
    "    def train(self, input_array, target_array):\n",
    "        return Loss(input_array, target_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开mnist数据集，加载到一个60000 * 28 * 28 * 1的矩阵中\n",
    "# load the mnist data set into a matrix\n",
    "training_data_file = open(\"D:/python/neural_network/mnist_dataset/mnist_train.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "test_data_file = open(\"D:/python/neural_network/mnist_dataset/mnist_test.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "\n",
    "training_data_array = np.zeros((60000, 28, 28, 1))\n",
    "training_label_list = []\n",
    "for i, record in enumerate(training_data_list):\n",
    "    all_values = record.split(',')\n",
    "    training_data_array[i, :, :, :] = np.asfarray(all_values[1:]).reshape(28, 28, 1)\n",
    "    training_label_list.extend(all_values[0])\n",
    "training_data_array /= 255.0\n",
    "training_data_array *= 0.99\n",
    "training_data_array += 0.01\n",
    "\n",
    "\n",
    "del training_data_list\n",
    "\n",
    "test_data_array = np.zeros((10000, 28, 28, 1))\n",
    "test_label_list = []\n",
    "for i, record in enumerate(test_data_list):\n",
    "    all_values = record.split(',')\n",
    "    test_data_array[i, :, :, :] = np.asfarray(all_values[1:]).reshape(28, 28, 1)\n",
    "    test_label_list.extend(all_values[0])\n",
    "test_data_array /= 255.0\n",
    "test_data_array *= 0.99\n",
    "test_data_array += 0.01\n",
    "\n",
    "del test_data_list\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = 32\n",
    "class CNN:\n",
    "    def __init__(self):\n",
    "        self.layer1 = ConvLayer((minibatch, 28, 28, 1), (5, 5, 1, 6), 1, 0, 0.0003, True, True, False, 'layer1')\n",
    "        self.layer2 = PoolingLayer((minibatch, 24, 24, 6), 2, 2, 'max')\n",
    "        self.layer3 = ConvLayer((minibatch, 12, 12, 6), (3, 3, 6, 20), 1, 0, 0.0003, True, False, False, 'layer3')\n",
    "        self.layer4 = PoolingLayer((minibatch, 10, 10, 20), 2, 2, 'max')\n",
    "        self.layer5 = FullyConnectedLayer(500, 120, 0.1, minibatch, False, False, 'layer5')\n",
    "        self.layerx = BatchNormalizationLayer2D((120, 32), 0.1, False, 'layerx')\n",
    "        self.layer6 = FullyConnectedLayer(120, 80, 0.1, minibatch, False, False, 'layer6')\n",
    "        self.layer7 = LastConnectedLayer(80, 10, 0.1, minibatch, False, False, 'layer7')\n",
    "        self.layer8 = SoftMax()\n",
    "        \n",
    "    def query(self, input_array):\n",
    "        out1 = self.layer1.fp(input_array)\n",
    "        out2 = self.layer2.fp(out1)\n",
    "        out3 = self.layer3.fp(out2)\n",
    "        out4 = self.layer4.fp(out3).reshape(500, 1)\n",
    "        out5 = self.layer5.fp(out4)\n",
    "        outx = self.layerx.fp(out5)\n",
    "        out6 = self.layer6.fp(outx)\n",
    "        out7 = self.layer7.fp(out6)\n",
    "        out8 = self.layer8.fp(out7)\n",
    "        return out8\n",
    "    def train(self, input_array, target_array):\n",
    "        out1 = self.layer1.fp(input_array)\n",
    "        out2 = self.layer2.fp(out1)\n",
    "        out3 = self.layer3.fp(out2)\n",
    "        out4 = self.layer4.fp(out3).reshape(minibatch, 500).T\n",
    "        out5 = self.layer5.fp(out4)\n",
    "        outx = self.layerx.fp(out5)\n",
    "        out6 = self.layer6.fp(outx)\n",
    "        out7 = self.layer7.fp(out6)\n",
    "        error1 = self.layer8.train(out7, target_array)\n",
    "        error2 = self.layer7.bp(out6, error1)\n",
    "        error3 = self.layer6.bp(outx, error2)\n",
    "        errorx = self.layerx.bp(error3)\n",
    "        error4 = self.layer5.bp(out4, errorx).T.reshape(minibatch, 5, 5, 20)\n",
    "        error5 = self.layer4.bp(error4)\n",
    "        error6 = self.layer3.bp(out2, error5)\n",
    "        error7 = self.layer2.bp(error6)\n",
    "        error8 = self.layer1.bp(input_array, error7)\n",
    "    \n",
    "    # 用于可视化，可忽略\n",
    "    # Visualization, can be ignored\n",
    "    def layer1bp(self, input_array, n):\n",
    "        out1 = self.layer1.fp(input_array)\n",
    "        \n",
    "        for i in range(out1.shape[3]):\n",
    "            if n == i:\n",
    "                continue\n",
    "            else:\n",
    "                out1[:, :, :, i] = 0\n",
    "        xxxx = self.layer1.backQuery(out1)\n",
    "        return xxxx\n",
    "    \n",
    "    # 用于可视化，可忽略\n",
    "    # Visualization, can be ignored\n",
    "    def layer2bp(self, input_array, n):\n",
    "        out1 = self.layer1.fp(input_array)\n",
    "        out2 = self.layer2.fp(out1)\n",
    "        \n",
    "        for i in range(out2.shape[3]):\n",
    "            if n != i:\n",
    " \n",
    "                out2[:, :, :, i] = 0\n",
    "        x1 = self.layer2.backQuery(out2)\n",
    "        x2 = self.layer1.backQuery(x1)\n",
    "        return x2\n",
    "    \n",
    "    # 用于可视化，可忽略\n",
    "    # Visualization, can be ignored\n",
    "    def layer3bp(self, input_array, n):\n",
    "        #fig = plt.figure()\n",
    "        out1 = self.layer1.fp(input_array)\n",
    "        out2 = self.layer2.fp(out1)\n",
    "        out3 = self.layer3.fp(out2)\n",
    "        \n",
    "        for i in range(out3.shape[3]):\n",
    "            if n == i:\n",
    "                continue\n",
    "            else:\n",
    "                out3[:, :, :, i] = 0\n",
    "\n",
    "        x0 = self.layer3.backQuery(out3)\n",
    "        x1 = self.layer2.backQuery(x0)\n",
    "        x2 = self.layer1.backQuery(x1)\n",
    "        \n",
    "        return x2\n",
    "    \n",
    "    def saveData(self, path):\n",
    "        self.layer1.saveKernel(path)\n",
    "        self.layer3.saveKernel(path)\n",
    "        self.layer5.saveWeight(path)\n",
    "        self.layer6.saveWeight(path)\n",
    "        self.layer7.saveWeight(path)\n",
    "        self.layerx.saveBNParameter(path)\n",
    "        \n",
    "    def loadData(self, path):\n",
    "        self.layer1.loadKernel(path)\n",
    "        self.layer3.loadKernel(path)\n",
    "        self.layer5.loadWeight(path)\n",
    "        self.layer6.loadWeight(path)\n",
    "        self.layer7.loadWeight(path)\n",
    "        self.layerx.loadBNParameter(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-10, 0, 10] #图片旋转角度 Image rotation Angle\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "q = 0\n",
    "minibatch = 32\n",
    "\n",
    "# 训练\n",
    "input_array = np.zeros((minibatch, 28, 28, 1))\n",
    "target_list = np.zeros((10, minibatch))\n",
    "for n in range(60000):\n",
    "    inputs = training_data_array[n, :, :, :]\n",
    "    inputs = sni.rotate(inputs.reshape(28, 28), random.sample(a, 1)[0], cval=0.01, reshape=False)\n",
    "    RandomNoise(inputs)\n",
    "    input_array[i, :, :, 0] = inputs\n",
    "    #print(np.max(inputs))\n",
    "    #print(np.min(inputs))\n",
    "    targets = np.zeros(10)\n",
    "    targets[int(training_label_list[n])] = 1\n",
    "    target_list[:, i] = targets\n",
    "    i += 1\n",
    "    if i == minibatch:\n",
    "        cnn.train(input_array, target_list)\n",
    "        i = 0\n",
    "    j += 1    \n",
    "    if j == 100:\n",
    "        j = 0\n",
    "        print(k)\n",
    "        print(cnn.layer1.kernel[0, 0, 0, 0])\n",
    "        #print(cnn.layery.variance[0, 0, 0, 0])\n",
    "        print(cnn.layer3.kernel[0, 0, 0, 0])\n",
    "        print(cnn.layer5.weight[0, 0])\n",
    "        print(cnn.layer7.weight[0, 0])\n",
    "        #print(cnn.layerx.variance[0, 0])\n",
    "        k += 1\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scorecard = [] \n",
    "for i, correct_label in enumerate(test_label_list[0*1000:10*1000]): \n",
    "    inputs = test_data_array[i, :, :, :]\n",
    "    inputs = inputs.reshape(1, 28, 28, 1) \n",
    "    outputs = cnn.query(inputs) \n",
    "    #print(outputs)\n",
    "    label = np.argmax(outputs) \n",
    "    #print(label, correct_label)\n",
    "\n",
    "    if(label == int(correct_label)): \n",
    "        scorecard.append(1) \n",
    "    else: \n",
    "        scorecard.append(0)\n",
    "\n",
    "#print(\"test_T_F_list\", scorecard) \n",
    "scorecard_array = np.asarray(scorecard) \n",
    "print(\"performance = \", scorecard_array.sum() / scorecard_array.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-10, 0, 10]\n",
    "path = 'D:\\\\python\\\\neural_network\\\\parameter37\\\\count_'\n",
    "counter = 0\n",
    "while 1 :\n",
    "    a = [-10, 0, 10]\n",
    "    i = 0\n",
    "    j = 0\n",
    "    k = 0\n",
    "    q = 0\n",
    "    minibatch = 32\n",
    "\n",
    "    # 训练\n",
    "    input_array = np.zeros((minibatch, 28, 28, 1))\n",
    "    target_list = np.zeros((10, minibatch))\n",
    "    for n in range(60000):\n",
    "        inputs = training_data_array[n, :, :, :]\n",
    "        inputs = sni.rotate(inputs.reshape(28, 28), random.sample(a, 1)[0], cval=0.01, reshape=False)\n",
    "        RandomNoise(inputs)\n",
    "        input_array[i, :, :, 0] = inputs\n",
    "        #print(np.max(inputs))\n",
    "        #print(np.min(inputs))\n",
    "        targets = np.zeros(10)\n",
    "        targets[int(training_label_list[n])] = 1\n",
    "        target_list[:, i] = targets\n",
    "        i += 1\n",
    "        if i == minibatch:\n",
    "            cnn.train(input_array, target_list)\n",
    "            i = 0\n",
    "\n",
    "    scorecard = [] \n",
    "    for i, correct_label in enumerate(test_label_list): \n",
    "        inputs = test_data_array[i, :, :, :]\n",
    "        inputs = inputs.reshape(1, 28, 28, 1) \n",
    "        outputs = cnn.query(inputs) \n",
    "        #print(outputs)\n",
    "        label = np.argmax(outputs) \n",
    "        #print(label, correct_label)\n",
    "\n",
    "        if(label == int(correct_label)): \n",
    "            scorecard.append(1) \n",
    "        else: \n",
    "            scorecard.append(0)\n",
    "\n",
    "    #print(\"test_T_F_list\", scorecard) \n",
    "    scorecard_array = np.asarray(scorecard) \n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    print(\"performance = \", scorecard_array.sum() / scorecard_array.size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mkdir(path + str(counter))\n",
    "    cnn.saveData(path + str(counter) + '\\\\')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #read_mat = np.loadtxt(filename, dtype=np.float64, delimiter=',')\n",
    "    #print(read_mat) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:\\\\python\\\\neural_network\\\\parameter_zero\\\\count_'\n",
    "counter = 29\n",
    "cnn.loadData(path + str(counter) + '\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
